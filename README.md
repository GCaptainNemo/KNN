# K近邻模型(KNN)
## 一、介绍
K近邻法是一种基本的分类和回归方法，这里只介绍分类问题中的KNN算法。分类时，对新的实例，根据K个最近邻的数据点的类别
通过多数表决等方式进行预测。KNN模型并没有待学习的参数，通过训练数据集将样本空间进行了一个划分，并作为分类的模型。
在K值得选取方面，当K值减小的时候，决策边界会变得复杂，也更容易过拟合；而当K值增大时，模型的复杂度会降低，但预测精度会降低。
在应用中，k值一般取一个较小的数值，通常采用交叉验证法来选取最优的K值。

## 二、Kd-tree
在实现KNN算法时，主要考虑如何对训练数据进行快速k近邻搜索。kd树(k-dimension tree)是一种对k维空间中的数据点进行存储以便对其进行快速检索的数据结构。
在数据点为一维的情况下，kd树就是二分查找(Binary Search Tree, BST)。

### 2.1 建立kd树

建立kd树的步骤如下所示：
1. 选取方差最大的维度作为分割维度；
2. 选择该维度的中位数作为分割点；
3. 将训练集中该维度小于中位数的划分到分割点的左子树，大于中位数的划分到分割点的右子树；
4. 重复执行步骤1-3，直至所有数据都被建立到KD Tree的节点上为止。

也可以不选择方差最大的维度作为分隔维度，而是循环遍历每一个维度。每次选择中位数作为分割点会产生一个平衡的k-d树。每个叶节点的高度都十分接近。
然而，**平衡的树不一定对每个应用都是最佳的**。Kd-tree实质上是对数据点所在的空间用垂直于坐标轴的超平面进行了一个**划分**。

### 2.2 最近邻搜索
最邻近搜索用来找出在树中与输入点最接近的点，k-d树最邻近搜索(深度优先)过程如下：
1. 构造一个栈数据结构。
2. 从根节点开始，通过二叉树搜索，如果节点的分割维度值小于查找点的维度值表示查找点位于左子树中，则进入左子树，否则进入右子树，
直到达到叶子节点为止。将经过的每一个节点都加入搜索路径(压栈)。
2. 回溯搜索路径(出栈)，判断以当前节点的兄弟节点为根节点的子树中是否可能有距离搜索点更近的节点。
如果有则将该兄弟节点加入到搜索路径中(压栈)。
3. 重复3直到搜索路径为空，输出距离搜索点最近的节点。
利用kd-tree进行搜索可以形象地用下面这张图来表示：

![kd_tree_search](resources/kd_tree_search.png)

### 2.3 K近邻搜索
k近邻搜索的思路是：
1. 首先构建空的最大堆（列表）。
2. 从根节点出发，计算当前节点与输入实例的距离，若最大堆元素小于k个，则将距离插入最大堆中，否则比较该距离是否小于堆顶距离值，若小于，则使用该距离替换堆顶元素；
3. 递归的遍历kd树中的节点，通过如下方式控制进入分支：
    a. 若堆中元素小于k个或该节点中的样本点与输入实例形成的超球体包含堆顶样本点，则进入左右子节点搜索；
    b. 否则，若输入实例当前维的坐标小于该节点当前维的坐标，则进入左子节点搜索；
    c. 否则，进入右子节点搜索；
4. 当到达叶节点时，搜索结束。最后最大堆中的k个节点，即为输入实例的k近邻点。
